# Original created by Mai Chu from Ageas, modified and commented by Yong on 07/01/2019

# Clean the workspace -----------------------------------------------------
rm(list = ls()); cat("\014")


# Load the packages -------------------------------------------------------
library("rvest")
library("tm")
library("wordcloud")


# html file to be web-scraped ----------------------------------------------------
url <- "https://www.quora.com/Why-is-insurance-necessary"
webpage <- read_html(url)
node <- html_nodes(webpage, ".ui_qtext_expanded , .ui_qtext_para, .ui_qtext_rendered_qtext")
# the second argument to the html_nodes() are the nodes, you can find the nodes with the Selectorgadget, it's an extension to add to the Google Chrome
text <- html_text(node)
class(text) # the text object is a character vector, it needs to be converted as corpus (a collection of written text), the functions VectorSource() and Corpus from the package "tm" can be used:

## Loading the required package: NLP
## Load the data as Corpus
docs <- Corpus(VectorSource(text))

## inspect the content: if you want to see the content
inspect(docs)


# Text transformation and cleaning -----------------------------------------------------
# transformation is performed using the tm_map() function to replace special character from the text. 
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))

docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
docs <- tm_map(docs, toSpace, "Ã¢")
docs <- tm_map(docs, toSpace, "???")
# the tm_map func is used to remove unnecessary white space, to convert the text to lower case, to remove common stopwords such as "the", "we".
# For "stopwords", supoported languages are danish, dutch, english, finish, german, hungarian, italian, norwegian, portuguese, russian, spanish, and swedish. Language names are case sensitive.


# Cleaning the text -------------------------------------------------------
# Convert the text to lower case
docs <- tm_map(docs, content_transformer(tolower))
# Remove numbers
docs <- tm_map(docs, removeNumbers)
# Remove english common stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
# Remove your own stop word
# specify your stopwords as a character vector
docs <- tm_map(docs, removeWords, c("blabla1", "blabla2")) 
# Remove punctuations
docs <- tm_map(docs, removePunctuation)
# Eliminate extra white spaces
docs <- tm_map(docs, stripWhitespace)
# Text stemming
# docs <- tm_map(docs, stemDocument)

### generate the word cloud and frequency plot
# before generating the cloud, we build the term-document matrix. column names are the words and row names are documents. the function TermDocumentMatrix() from the "tm" package is used. 
# Build a term-document matrix --------------------------------------------

dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)


# Generate the Word cloud -------------------------------------------------
# the cloud can be generated using the func wordcloud() from the package "wordcloud"
set.seed(1234)
X11();
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
          max.words=200, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))

## without taking into account the insurance word
# after removing the word "insurance": 
X11();
wordcloud(words = d$word[-1], freq = d$freq[-1], min.freq = 1,
          max.words=200, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))


# Explore frequent terms and their associations ---------------------------
findFreqTerms(dtm, lowfreq = 4)
# to find words that occur at least 4 times
findAssocs(dtm, terms = "insurance", corlimit = 0.3) # analyze the association between frequent terms (i.e. terms which correlate) using the findAssocs() function. Here we identify which words are associated with "insurance"

# Plot word frequencies ---------------------------------------------------

X11();
barplot(d[1:10,]$freq, las = 2, names.arg = d[1:10,]$word,
        col ="lightsalmon", main ="Most frequent words",
        ylab = "Word frequencies")
